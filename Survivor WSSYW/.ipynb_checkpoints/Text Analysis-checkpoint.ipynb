{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analysis\n",
    "==\n",
    "This project is a text analysis of the r/survivor subreddit's opinion on each Survivor season. The goal of this visualization is to identify main descriptors for each season. Text analysis inspired by [github/walkerkq](https://github.com/walkerkq/textmining_southpark/)\n",
    "\n",
    "All data sourced from [r/survivor](https://www.reddit.com/r/survivor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------- Reading the Data -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Reddit API\n",
    "Establish connection to Reddit via the Python Reddit API Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "# Track date of data pull\n",
    "pull_date = datetime.now().strftime(\"%b %d %Y %H:%M:%S\")\n",
    "\n",
    "# Create connection to Reddit via PRAW\n",
    "fp = open('./settings.json')\n",
    "settings = json.load(fp).get('praw')\n",
    "reddit = praw.Reddit(client_id = settings.get('client_id'),\n",
    "                    client_secret = settings.get('client_secret'),\n",
    "                    user_agent = settings.get('user_agent'),\n",
    "                    username = settings.get('username'),\n",
    "                    password = settings.get('password'))\n",
    "subreddit = reddit.subreddit('survivor')\n",
    "\n",
    "fp = open('./seasons.json')\n",
    "seasons = json.load(fp)\n",
    "\n",
    "readData = False\n",
    "processTopPosts = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Reddit Submission IDs\n",
    "Get the reddit ID of all posts to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 100 submissions on each season\n",
    "if(processTopPosts):\n",
    "    top_submissions = {}\n",
    "    for (nbr, season) in seasons.items():\n",
    "        submissions = subreddit.search(\"flair:\"+season.replace(\"\\n\",\" \"),sort='top')\n",
    "        for sub in submissions:\n",
    "            top_submissions[sub.id] = nbr\n",
    "\n",
    "# Unspoiled posts\n",
    "wssyw_ids = ['10tpq9', # e.g. S06: The Amazon, hyperlinked\n",
    "             '26viy4', # e.g. Season 28: Cagayan\n",
    "             '37c7zu', # e.g. S15: China\n",
    "             '3xp433', \n",
    "             '4kzd4g', \n",
    "             '5jwzib', \n",
    "             '6ga0ty', # [Season 7: Pearl Islands](link to countdown)\n",
    "             '8p0ye9', \n",
    "             'btu8iu'  # Note: Still posting countdown\n",
    "            ]\n",
    "wssyw_spoilers = {'1':'c48snf',\n",
    "                  '2':'c0mplt',\n",
    "                  '3':'c1c9cs',\n",
    "                  '4':'c1r9tn',\n",
    "                  '5':'bwr435',\n",
    "                  '6':'c3slsk',\n",
    "                  '7':'c8c3ov',\n",
    "                  '8':'bx4u7z',\n",
    "                  '9':'c2jn8g',\n",
    "                  '10':'c24mgl',\n",
    "                  '11':'c0ep8t',\n",
    "                  '12':'c5xgnr',\n",
    "                  '13':'c12yug',\n",
    "                  '14':'by9sit',\n",
    "                  '15':'c8bzdu',\n",
    "                  '16':'c6b0ce',\n",
    "                  '17':'c5pfk8',\n",
    "                  '18':'c8c1g8',\n",
    "                  '19':'bymej8',\n",
    "                  '20':'c82s4d',\n",
    "                  '21':'bztmw5',\n",
    "                  '22':'bv9dna',\n",
    "                  '23':'bzequv',\n",
    "                  '24':'bwee06',\n",
    "                  '25':'c6bmtt'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to extract data\n",
    "Define functions that will help get the reddit comment tree as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of all comments\n",
    "def get_reply_comments(submission):\n",
    "    d = []\n",
    "    for top_level_comment in submission.comments:\n",
    "        if (not top_level_comment.is_submitter):\n",
    "            continue    \n",
    "        season = None\n",
    "        season_name = None\n",
    "        match1 = re.search(\"(\\d+:( (\\w|-)+|\\.+){1,})\",top_level_comment.body)\n",
    "        match2 = re.search(\"(:(\\d+)( (\\w|-)+|\\.+){1,})\",top_level_comment.body) # Data error of S:00 Example Name\n",
    "        if match1:\n",
    "            season = int(match1.group(0).split(': ')[0])\n",
    "            season_name = seasons[str(season)]\n",
    "        if match2:\n",
    "            season = int(match2.group().split(':')[1].split(' ')[0])\n",
    "            season_name = seasons[str(season)]\n",
    "        for comment in top_level_comment.replies.list():\n",
    "            d.append({'season_nbr':season,'season':season_name,'comment_id':comment.id,'comment':comment.body,'score':comment.score})\n",
    "    return d\n",
    "\n",
    "# Get comments for each WSSYW Countdown submission\n",
    "def getComments(r_id, top_level_comment=False, season_key=0):\n",
    "    d = []\n",
    "    submission = reddit.submission(id=r_id)\n",
    "    submission.comments.replace_more(limit=2500, threshold=0)\n",
    "    if top_level_comment:\n",
    "        [d.append(row) for row in get_reply_comments(submission)]\n",
    "    else:\n",
    "        for comment in submission.comments.list():\n",
    "            d.append({'season_nbr':int(season_key),'season':seasons[season_key],'comment_id':comment.id,'comment':comment.body,'score':comment.score})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (readData):\n",
    "    #### LARGE DATA SET ####\n",
    "    if (processTopPosts):\n",
    "        d_all = []\n",
    "        for (r_id,season_nbr) in top_submissions.items():\n",
    "            [d_all.append(row) for row in getComments(r_id,False,season_nbr)]\n",
    "        df_raw_all = pd.DataFrame(d_all)\n",
    "\n",
    "        # Save csv for reference\n",
    "        df_raw_all.to_csv('./data/top_post_comments.csv',index = None, header=True)\n",
    "\n",
    "    #### UNSPOILED DATA ####\n",
    "    d=[]\n",
    "    for r_id in wssyw_ids:\n",
    "        [d.append(row) for row in getComments(r_id, True)]\n",
    "\n",
    "    df_raw_wssyw = pd.DataFrame(d)\n",
    "\n",
    "    # Save csv for reference\n",
    "    df_raw_wssyw.to_csv('./data/unspoiled_comments.csv',index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe based on results of reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_read = \"./data/unspoiled_comments.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(csv_to_read)\n",
    "df_raw_clean = df_raw[df_raw['score']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import OrderedDict\n",
    "\n",
    "s = \"'' think deserve mention breath ri caramoan .\"\n",
    "t = sent_tokenize(s)\n",
    "seen = set()\n",
    "print([len(word_tokenize(sent)) for sent in t])\n",
    "print( [x for x in t if (x not in seen and not seen.add(x)) or (len(word_tokenize(x))<4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------- Cleaning the Data -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = ['i', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "        \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "        'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "        'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', \n",
    "        'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'a', 'an', 'the', 'because', \n",
    "        'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "        'during', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
    "        'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',  'some', 'such',  's', 't', \n",
    "        'will', 'just', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain']\n",
    "from textblob import Word\n",
    "from collections import OrderedDict\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "fp = open('./contractions.json')\n",
    "contractions = json.load(fp)\n",
    "lem = WordNetLemmatizer() \n",
    "\n",
    "def get_wordnet_pos(sent):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    sent_pos = []\n",
    "    tags = pos_tag(word_tokenize(sent))\n",
    "    for tag in tags:\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        sent_pos += [tag_dict.get(tag[1][0], wordnet.NOUN)]\n",
    "    return sent_pos\n",
    "\n",
    "def remove_copy_paste(x):\n",
    "    sentences = sent_tokenize(x)\n",
    "    seen = set()\n",
    "    return \" \".join([sent for sent in sentences if (sent not in seen and not seen.add(sent)) or (len(word_tokenize(sent))<8)])\n",
    "def to_lower(x):\n",
    "    return \" \".join(x.lower() for x in x.split())\n",
    "def remove_stop(x):\n",
    "    return \" \".join(word for word in x.split() if word not in stop)\n",
    "def lem_words(x):\n",
    "    contr = \" \".join([contractions.get(word.replace(\"'\",''), word.replace(\"'\",'')) for word in x.split()])\n",
    "    pos = get_wordnet_pos(contr)\n",
    "    lems = []\n",
    "    for w,p in zip(word_tokenize(contr), pos):\n",
    "        lems+=[lem.lemmatize(w, p)]\n",
    "    return \" \".join(lems)\n",
    "\n",
    "df_clean_tmp = df_raw_clean\n",
    "col = df_raw_clean['comment'].apply(lambda x: to_lower(x)).apply(lambda x: lem_words(x)).str.replace('[^\\w\\s]|[\\d]','') #.apply(lambda x: remove_stop(x))\n",
    "df_clean_tmp = df_raw_clean.assign(comment_clean=col.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine cleaned data into one row for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = pd.DataFrame(columns = ['comment'])\n",
    "\n",
    "df_comb['comment'] = df_clean_tmp.groupby('season_nbr')['comment_clean'].apply(lambda x: \" \".join(x))\n",
    "df_comb['season_nbr'] = df_comb.index\n",
    "\n",
    "df_clean = df_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of proper nouns based on sentence structure of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "\n",
    "def getPropNouns(data):\n",
    "    chunks = ne_chunk(pos_tag(word_tokenize(data)))\n",
    "    nouns = []\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if(chunk.label()==\"PERSON\"):\n",
    "                nouns+=[chunk[0][0].lower()]\n",
    "    return nouns\n",
    "\n",
    "def getIgnoreWords(data):\n",
    "    tags = pos_tag(word_tokenize(data))\n",
    "    tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "    words = []\n",
    "    for tag in tags:\n",
    "        if (tag_dict.get(tag[1][0], wordnet.NOUN) != wordnet.NOUN):\n",
    "           words+=[tag[0].lower()]\n",
    "    return words\n",
    "        \n",
    "non_nouns = getIgnoreWords(\" \".join(df_raw['comment']))\n",
    "\n",
    "propNounsTemp = {}\n",
    "df_raw['entity'] = df_raw['comment'].apply(lambda x: getPropNouns(x))\n",
    "for row in df_raw['entity']:\n",
    "    for item in row:\n",
    "        propNounsTemp[item] = propNounsTemp.get(item, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propNouns=[]\n",
    "for (key, count) in propNounsTemp.items():\n",
    "    if count > 3:\n",
    "        propNouns+=[key]\n",
    "# print([word for word in propNouns if pos_tag([word])[0][1][0] in ['N']])\n",
    "propNouns = [word for word in propNouns if word not in non_nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_words = pd.Series(' '.join(df_clean['comment']).split()).value_counts()[pd.Series(' '.join(df_clean['comment']).split()).value_counts()<=1].index.values.tolist()\n",
    "freq_words = pd.Series(' '.join(df_clean['comment']).split()).value_counts()[:100].index.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------- Determine Characteristic Words -----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the characteristic-ness of a word to a season\n",
    "Calculate the tf-idf values to determine how characteristic a word/phrase is to a given season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "drop_terms = rare_words + [word.replace(' ','') for word in stopwords.words('english')]\n",
    "# vec = CountVectorizer(ngram_range=(1,4), analyzer='word')\n",
    "vec = TfidfVectorizer(ngram_range=(1,3), analyzer='word', max_df=0.99)\n",
    "X = vec.fit_transform(df_clean['comment'])\n",
    "df_tmp = pd.DataFrame(X.toarray(), index=df_clean['season_nbr'] ,columns=vec.get_feature_names())\n",
    "tdm = pd.DataFrame(df_tmp.transpose())\n",
    "tdm = tdm[~tdm.index.isin(drop_terms)]\n",
    "tdm.index.name = 'word'\n",
    "tdm['sum'] = tdm.sum(axis=1)\n",
    "tdm = tdm[~tdm.index.isin(tdm.sort_values(24.0, ascending=False)[:26][tdm[24.0]==tdm['sum']].index.tolist())]\n",
    "\n",
    "tdm.sort_values('sum', ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine sentiment of words in tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = [TextBlob(word) for word in tdm.index.tolist()]\n",
    "tdm['sentiment'] = [float(b.sentiment.polarity) for b in blob]\n",
    "tdm['subjective'] = [float(b.sentiment.subjectivity) for b in blob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ranked dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_empty(columns, dtypes, index=None):\n",
    "    assert len(columns)==len(dtypes)\n",
    "    df = pd.DataFrame(index=index)\n",
    "    for c,d in zip(columns, dtypes):\n",
    "        df[c] = pd.Series(dtype=d)\n",
    "    return df\n",
    "ranked = df_empty(columns = ['season','season_name','rank','word','sentiment'],dtypes=[np.float64,np.str,np.float64,np.str,np.str])\n",
    "for col in tdm.drop(columns=['sentiment','subjective','sum']).columns:\n",
    "    df_new = df_empty(columns = ['season','season_name','rank','word','sentiment'],dtypes=[np.float64,np.str,np.float64,np.str,np.str])\n",
    "    df_new['word'] = tdm.sort_values(col, ascending=False).index.tolist()\n",
    "    df_new['rank'] = df_new.index\n",
    "    df_new['rank'] = pd.to_numeric(df_new['rank'])\n",
    "    df_new['season'] = float(col)\n",
    "    df_new['season_name'] = seasons.get(str(int(col)))\n",
    "    df_new['sentiment'] = tdm.sort_values(col, ascending=False)['sentiment'].apply(lambda x: 'very positive' if x >= 0.5 else 'positive' if 0.5 > x > 0.15 else 'very negative' if x <= -0.5 else 'negative' if -0.5 < x < -0.05 else 'neutral').tolist()\n",
    "    ranked = pd.concat([ranked,df_new[:26]])\n",
    "ranked.to_csv('./data/ranked_words.csv',index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------- Plot the Data -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "# Create graphic\n",
    "# Define colors\n",
    "color_background = \"white\"\n",
    "color_text = \"#22211d\"\n",
    "\n",
    "my_theme = (\n",
    "\n",
    "    # Begin construction of chart\n",
    "    theme_bw(base_size=15) +\n",
    "\n",
    "    # Format background colors\n",
    "    theme(panel_background = element_rect(fill=color_background, color=color_background)) +\n",
    "    theme(plot_background  = element_rect(fill=color_background, color=color_background)) +\n",
    "    theme(panel_border     = element_rect(color=color_background)) +\n",
    "    theme(strip_background = element_rect(fill=color_background, color=color_background)) +\n",
    "\n",
    "    # Format the grid\n",
    "    theme(panel_grid_major_y = element_blank()) +\n",
    "    theme(panel_grid_minor_y = element_blank()) +\n",
    "    theme(axis_ticks         = element_blank()) +\n",
    "\n",
    "    # Format the legend\n",
    "    theme(legend_position = \"none\") +\n",
    "\n",
    "    # Format title and axis labels\n",
    "    theme(plot_title       = element_text(color=color_text, size=20, weight = \"bold\")) +\n",
    "    theme(axis_title_x     = element_text(size=10, color=\"black\", weight = \"bold\")) +\n",
    "    theme(axis_title_y     = element_text(size=10, color=\"black\", weight = \"bold\")) +\n",
    "    theme(axis_text_x      = element_text(size=10, vjust=0.5, hjust=0.5, color = color_text)) +\n",
    "    theme(axis_text_y      = element_text(size=12, color = color_text)) +\n",
    "    theme(strip_text       = element_text(face = \"bold\")) +\n",
    "\n",
    "    # Plot margins\n",
    "    theme(figure_size = (18,6))\n",
    "    )\n",
    "\n",
    "cmap_era = {\"1.0\":\"#F70020\",\n",
    "            \"2.0\":\"#1A7D00\",\n",
    "            \"3.0\":\"#0C96F2\",\n",
    "            \"4.0\":\"#FB9701\",\n",
    "            \"5.0\":\"#636666\",\n",
    "            \"6.0\":\"#87603E\",\n",
    "            \"7.0\":\"#BFBEBB\",\n",
    "            \"8.0\":\"#FB9701\",\n",
    "            \"9.0\":\"#636666\",\n",
    "            \"10.0\":\"#87603E\"\n",
    "           }\n",
    "cmap_tone = {\"very positive\":\"#4CAF50\",\"positive\":\"#8BC34A\",\"neutral\":\"#B3B6B7\",\"negative\":\"#FFC107\",\"very negative\":\"#F4511E\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_fig(data, fill_col):\n",
    "    season_names = pd.Categorical(data['season_name'], categories=data['season_name'].unique().tolist())\n",
    "    data = data.assign(season_name_ord = season_names)\n",
    "    return(\n",
    "        ggplot(data=data, mapping=aes(x='season_name_ord', y='rank')) +\n",
    "        geom_point(color=\"black\") + \n",
    "        geom_label(data=data, \n",
    "                   mapping=aes(label='word', fill=fill_col), \n",
    "                   color='white',\n",
    "                   size=10,\n",
    "                   label_padding=0.15) +\n",
    "        geom_label(data=data.loc[data['word'].isin(propNouns)], \n",
    "           mapping=aes(label='word'), \n",
    "           fill='white',\n",
    "           color='grey',\n",
    "           size=10,\n",
    "           label_padding=0.15) +\n",
    "        scale_y_reverse(limits=[1,25], breaks=range(25,0,-1)) +\n",
    "        labs(x=\"Season\", y=\"Ranking\",title=\"Most Characteristic Words Used to Describe a Season\") +\n",
    "        scale_fill_manual(cmap_tone)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_10 = ranked[ranked['season']<11]\n",
    "fig1 = (base_fig(first_10, 'sentiment') + my_theme)\n",
    "ggsave(plot=fig1, filename='./images/seasons1to10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_10 = ranked[(ranked['season']>10) & (ranked['season']<21)]\n",
    "fig2 = (base_fig(second_10, 'sentiment') + \n",
    "        my_theme  \n",
    "       )\n",
    "ggsave(plot=fig2, filename='./images/seasons11to20.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_10 = ranked[(ranked['season']>20) & (ranked['season']<31)]\n",
    "fig3 = (base_fig(third_10, 'sentiment') + my_theme )\n",
    "ggsave(plot=fig3, filename='./images/seasons21to30.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_10 = ranked[(ranked['season']>30)]\n",
    "fig4 = (base_fig(fourth_10, 'sentiment') + my_theme )\n",
    "ggsave(plot=fig4, filename='./images/seasons31to38.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
